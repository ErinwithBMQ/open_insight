假如你是一位开源项目技术领域的专家，接下来会有一个任务，任务的目标是针对一个开源项目的提交记录进行与技术领域分类有关的内容，提取成特征范式的形式。请你判断提取一下这个输入，repo_name:https://github.com/pytorch/pytorchversion:v1.13.1, release_data:['This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n'], PR_data:['[skylight][caffe2][buck2][ondemand] Disable avx2 temporary for OnDemand x86_64 builds', '[RFC] Intel GPU Inductor backend upstreaming', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Dump sarif diagnostics for failed onnx exports in benchmark', '[ONNX] Dump sarif diagnostics for failed onnx exports in benchmark', '[ONNX] Add proper iobinding synchronize for ONNX cuda bench', '[ONNX] Add proper iobinding synchronize for ONNX cuda bench', '[dynamo] Log shape_env_guard_count separately from guard_count'], commit_data:['[C10d] Cleanup collective sequence number. (#109136)\n\nSequence numbers must be associated with a Work object\nif we want to use it as a way to report collective progress.\n\nThe API surface change is introducing Work::getSequenceNumber, which\nshould eventually be exposed to python.\n\nThe bulk of this change is changing gloo to make the sequence number\nbe always in use and weave it to the dozens subclasses of Work.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109136\nApproved by: https://github.com/fduwjj', '[executorch][kernel reg] Allow kernel manual registration (#110086)\n\nSummary:\nExposing a codegen mode for generating a hook for user to register their kernels.\n\nIf we pass `--manual-registration` flag to `gen_executorch.py`, we will generate the following files:\n1. RegisterKernels.h which declares a `register_all_kernels()` API inside `torch::executor` namespace.\n2. RegisterKernelsEverything.cpp which implements `register_all_kernels()` by defining an array of generated kernels.\n\nThis way user can depend on the library declared by `executorch_generated_lib` macro (with `manual_registration=True`) and be able to include `RegisterKernels.h`. Then they can manually call `register_all_kernels()` instead of relying on C++ static initialization mechanism which is not available in some embedded systems.\n\nTest Plan:\nRely on the unit test:\n\n```\nbuck2 test fbcode//executorch/runtime/kernel/test:test_kernel_manual_registration\n```\n\nReviewed By: cccclai\n\nDifferential Revision: D49439673\n\nPull Requ', '[Easy] use BaseListVariable cls_for for all list-y type dispatching (#110159)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110159\nApproved by: https://github.com/ezyang', 'Pass S3 credentials to ios upload workflow (#109222)\n\nThis fixes the failed upload to S3 for nightly and release build. The credentials needs to be passed from the caller workflow.  We also need to setup the credential in D49291627 before merging this one.\n\n### Testing\n\nUpload successfully https://github.com/pytorch/pytorch/actions/runs/6190836578/job/17125308432?pr=109222#step:13:51\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109222\nApproved by: https://github.com/atalman', 'add fp16 support for gemm (#99498)\n\n### Testing\n\nNative matmul vs. mkldnn matmul  on SPR (with avx512_fp16 support)\n\nsingle core:\n\nInput | Naïve impl   / ms | oneDNN /   ms | Speed up\n-- | -- | -- | --\nM: 128, N: 128, K: 128, trans_a: False, trans_b: False | 2010.387 | 64.700 | 31.072\nM: 128, N: 256, K: 128, trans_a: False, trans_b: False | 4027.116 | 107.780 | 37.364\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: False | 28685868.488 | 90663.008 | 316.401\n\n56 cores:\nInput | Naïve impl   / ms | oneDNN /   ms | Speed up\n-- | -- | -- | --\nM: 128, N: 128, K: 128, trans_a: False, trans_b: False | 5.091 | 0.24 | 211.30\nM: 128, N: 128, K: 128, trans_a: False, trans_b: True | 5.224 | 0.23 | 220.09\nM: 128, N: 256, K: 128, trans_a: False, trans_b: False | 10.006 | 0.30 | 330.31\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: False | 29435.372 | 1.770 | 1662.80\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: True | 31464.961 | 1.728 |  18204.76\nM: 8192, N: 768, K: 3072, trans_a: False, tr', '[core IR] Add lift_fresh, split.Tensor, and unbind decompositions to core ATen decomp table (#110102)\n\n## Context\n\nAdd existing decomps for `lift_fresh`, `split.Tensor`, and `unbind` to the core ATen decomposition table. Do not use them in inductor, since Inductor currently lowers these directly.\n\nOne note though is that `lift_fresh`\'s decomposition has a note saying it\'s not correct under autograd. However, my understanding is that these decompositions are registered to the `"post_autograd"` decomposition table, meaning autograd wouldn\'t be a factor. Would like some confirmation that this premise is correct.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110102\nApproved by: https://github.com/jansel', 'Revert "[1/N] Dynamo skipfiles refactor (#109567)"\n\nThis reverts commit f8e0ebec8c6156922026fc2bf6e5a829097b4506.\n\nReverted https://github.com/pytorch/pytorch/pull/109567 on behalf of https://github.com/huydhn due to Many jobs are failing in trunk after this with FILENAME_ALLOWLIST is not defined error https://hud.pytorch.org/pytorch/pytorch/commit/f8e0ebec8c6156922026fc2bf6e5a829097b4506. This looks like a landrace ([comment](https://github.com/pytorch/pytorch/pull/109567#issuecomment-1738344950))', '[AOTInductor] ProxyExecutor supports custom op with tuple output (#110140)\n\nSummary:\nExtend ProxyExecutor to support custom ops with tuple outputs.\n\nGenerated wrapper code for `out3, out4 = torch.ops.fb.fn_with_tuple_output(out2, 1)`\n\n```\n    AtenTensorHandle buf5_handle;  // output buffer\n    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&buf5_handle));\n    RAIIAtenTensorHandle buf5(buf5_handle);\n    AtenTensorHandle buf6_handle;  // output buffer\n    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&buf6_handle));\n    RAIIAtenTensorHandle buf6(buf6_handle);\n    AtenTensorHandle tensor_args_var_3[] = {buf3.get(), buf5.get(), buf6.get()};\n    int64_t int_args_var_4[] = {1};\n    aoti_torch_proxy_executor_call_function(proxy_executor, 1, 1, int_args_var_4, 3, tensor_args_var_3);\n```\n\nTest Plan: Test\n\nDifferential Revision: D49673994\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110140\nApproved by: https://github.com/chenyang78', 'Enable typechecking for _inductor/fx_passes/group_batch_fusion.py (#110111)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110111\nApproved by: https://github.com/eellison, https://github.com/Skylion007\nghstack dependencies: #110109', 'Reorganize and rename COW files and APIs (#110191)\n\nThis PR does the following:\n* Combine `cow/context.<h/cpp>` and `cow/deleter.<h/cpp>` into `cow/COWDeleter.<h/cpp>`\n* Rename `Context` to `COWDeleterContext`\n* Rename `delete_context` to `cow_deleter`\n* Remove the separate `impl_cow_context` bazel library, combining it with the base c10 core library\n* Rename `context_test.cpp` to `cow_test.cpp`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110191\nApproved by: https://github.com/ezyang'], language:'Python'，请提取出该开源项目的技术领域分类特征范式，包括技术领域分类名称、该分类的特征描述、该分类的特征提取依据、该分类的特征关键词。 作为专家，你的回答需要满足以下要求：

1. 你必须按照技术领域分类名称、特征描述、特征提取依据、特征关键词的顺序输出，每个分类作为一个条目，每条之间用换行符分隔。
2. 你必须使用中文进行输出。
3. 你必须确保每个技术领域分类名称和特征关键词是不同的，不能重复。
4. 你必须确保特征描述和特征关键词准确反映该分类的内容，不能遗漏重要信息。
5. 你必须确保每个分类的特征提取依据来自提供的输入内容，不能编造信息。
6. 你必须确保特征关键词是提取自输入内容中的关键术语，不能自行编造关键词。
7. 你必须确保每个分类的特征描述和特征关键词之间有明确的对应关系，不能出现描述与关键词不匹配的情况。
8. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类密切相关，不能出现无关信息。
9. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类有直接关系，不能出现间接或模糊的关系。
10. 你必须确保每个分类的特征提取依据清晰明确，能够支持该分类的特征描述和特征关键词。
11. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类的定义和范围一致。
12. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类的定义和范围一致。
13. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类的定义和范围一致。
14. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类的定义和范围一致。
15. 你必须确保每个分类的特征描述和特征关键词都与该技术领域分类的定义和范围一致。

好的，我现在需要处理用户的任务，提取给定开源项目的技术领域分类特征范式。首先，我要仔细阅读用户提供的输入内容，包括repo_name、version、release_data、PR_data、commit_data以及语言信息。用户希望我按照特定的格式输出，每个分类包括名称、特征描述、依据、关键词，并且要满足一系列严格的条件。

首先，我需要理解用户提供的输入内容。repo_name是PyTorch的GitHub仓库，版本是v1.13.1。release_data中有多个重复的发布说明，提到修复了各种问题，包括RuntimeError、安装问题、文档缺失、安全漏洞、测试用例修正、ONNX相关功能改进等。PR_data中的提交涉及多个技术领域，比如ONNX、Dynamo、Executorch、C++代码优化等。commit_data中有具体的提交信息，涉及分布式训练、内核注册、类型检查、性能优化等。

接下来，我需要从这些数据中提取技术领域分类。首先观察release_data中的问题类型，比如安装问题（pip、poetry、Amazon Linux 2、Mac M1）、文档缺失、安全漏洞（command injection）、ONNX相关功能、CUDA/CPU检查、反向传播错误等。这些可能对应不同的技术领域，如安装与部署、文档与维护、安全与漏洞、ONNX集成、性能优化、错误修复等。

然后看PR_data中的提交，例如ONNX相关的提交、Dynamo的日志记录、Executorch的内核注册、C++的类型检查、性能优化（如FP16支持）。这些可能对应到机器学习框架优化、编译器与执行引擎、跨平台支持、性能优化等。

commit_data中的提交涉及分布式训练（C10d）、内核注册、类型检查、性能优化（如GEMM的FP16支持）、代码重构等，这些可以归类到分布式计算、内核开发、类型系统、性能优化等。

接下来，我需要确保每个分类名称不重复，特征描述准确，关键词来自输入内容，并且依据明确。例如，安装与部署可能包括安装问题、跨平台支持（Mac M1、Windows）；安全与漏洞涉及command injection、Python frame安全使用；ONNX集成有ONNX ATen Fallback、operator_export_type等关键词。

需要逐一检查每个分类是否符合用户的要求，确保没有遗漏重要信息，关键词和描述对应，依据来自输入内容。例如，错误修复与调试可能涉及RuntimeError、double-backward错误、测试用例修正，这些在release_data和commit_data中都有提到。

可能遇到的困难是确保所有分类都正确无误，不重复，并且每个条目都符合用户的具体要求。需要多次核对，确保每个分类的依据确实来自提供的数据，关键词准确，描述与