假如你是一位开源项目技术领域的专家，接下来会有一个任务，任务的目标是针对一个开源项目的提交记录进行与技术领域分类有关的内容，提取成特征范式的形式.请不要输出问题，只给出问题的答案请你判断提取一下这个输入，repo_name:https://github.com/pytorch/pytorchversion:v1.13.1, release_data:['This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n', 'This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n'], PR_data:['[skylight][caffe2][buck2][ondemand] Disable avx2 temporary for OnDemand x86_64 builds', '[RFC] Intel GPU Inductor backend upstreaming', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Set correct cuda.current_device for multi-device onnx performance bench', '[ONNX] Dump sarif diagnostics for failed onnx exports in benchmark', '[ONNX] Dump sarif diagnostics for failed onnx exports in benchmark', '[ONNX] Add proper iobinding synchronize for ONNX cuda bench', '[ONNX] Add proper iobinding synchronize for ONNX cuda bench', '[dynamo] Log shape_env_guard_count separately from guard_count'], commit_data:['[C10d] Cleanup collective sequence number. (#109136)\n\nSequence numbers must be associated with a Work object\nif we want to use it as a way to report collective progress.\n\nThe API surface change is introducing Work::getSequenceNumber, which\nshould eventually be exposed to python.\n\nThe bulk of this change is changing gloo to make the sequence number\nbe always in use and weave it to the dozens subclasses of Work.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109136\nApproved by: https://github.com/fduwjj', '[executorch][kernel reg] Allow kernel manual registration (#110086)\n\nSummary:\nExposing a codegen mode for generating a hook for user to register their kernels.\n\nIf we pass `--manual-registration` flag to `gen_executorch.py`, we will generate the following files:\n1. RegisterKernels.h which declares a `register_all_kernels()` API inside `torch::executor` namespace.\n2. RegisterKernelsEverything.cpp which implements `register_all_kernels()` by defining an array of generated kernels.\n\nThis way user can depend on the library declared by `executorch_generated_lib` macro (with `manual_registration=True`) and be able to include `RegisterKernels.h`. Then they can manually call `register_all_kernels()` instead of relying on C++ static initialization mechanism which is not available in some embedded systems.\n\nTest Plan:\nRely on the unit test:\n\n```\nbuck2 test fbcode//executorch/runtime/kernel/test:test_kernel_manual_registration\n```\n\nReviewed By: cccclai\n\nDifferential Revision: D49439673\n\nPull Requ', '[Easy] use BaseListVariable cls_for for all list-y type dispatching (#110159)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110159\nApproved by: https://github.com/ezyang', 'Pass S3 credentials to ios upload workflow (#109222)\n\nThis fixes the failed upload to S3 for nightly and release build. The credentials needs to be passed from the caller workflow.  We also need to setup the credential in D49291627 before merging this one.\n\n### Testing\n\nUpload successfully https://github.com/pytorch/pytorch/actions/runs/6190836578/job/17125308432?pr=109222#step:13:51\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109222\nApproved by: https://github.com/atalman', 'add fp16 support for gemm (#99498)\n\n### Testing\n\nNative matmul vs. mkldnn matmul  on SPR (with avx512_fp16 support)\n\nsingle core:\n\nInput | Naïve impl   / ms | oneDNN /   ms | Speed up\n-- | -- | -- | --\nM: 128, N: 128, K: 128, trans_a: False, trans_b: False | 2010.387 | 64.700 | 31.072\nM: 128, N: 256, K: 128, trans_a: False, trans_b: False | 4027.116 | 107.780 | 37.364\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: False | 28685868.488 | 90663.008 | 316.401\n\n56 cores:\nInput | Naïve impl   / ms | oneDNN /   ms | Speed up\n-- | -- | -- | --\nM: 128, N: 128, K: 128, trans_a: False, trans_b: False | 5.091 | 0.24 | 211.30\nM: 128, N: 128, K: 128, trans_a: False, trans_b: True | 5.224 | 0.23 | 220.09\nM: 128, N: 256, K: 128, trans_a: False, trans_b: False | 10.006 | 0.30 | 330.31\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: False | 29435.372 | 1.770 | 1662.80\nM: 8192, N: 768, K: 768, trans_a: False, trans_b: True | 31464.961 | 1.728 |  18204.76\nM: 8192, N: 768, K: 3072, trans_a: False, tr', '[core IR] Add lift_fresh, split.Tensor, and unbind decompositions to core ATen decomp table (#110102)\n\n## Context\n\nAdd existing decomps for `lift_fresh`, `split.Tensor`, and `unbind` to the core ATen decomposition table. Do not use them in inductor, since Inductor currently lowers these directly.\n\nOne note though is that `lift_fresh`\'s decomposition has a note saying it\'s not correct under autograd. However, my understanding is that these decompositions are registered to the `"post_autograd"` decomposition table, meaning autograd wouldn\'t be a factor. Would like some confirmation that this premise is correct.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110102\nApproved by: https://github.com/jansel', 'Revert "[1/N] Dynamo skipfiles refactor (#109567)"\n\nThis reverts commit f8e0ebec8c6156922026fc2bf6e5a829097b4506.\n\nReverted https://github.com/pytorch/pytorch/pull/109567 on behalf of https://github.com/huydhn due to Many jobs are failing in trunk after this with FILENAME_ALLOWLIST is not defined error https://hud.pytorch.org/pytorch/pytorch/commit/f8e0ebec8c6156922026fc2bf6e5a829097b4506. This looks like a landrace ([comment](https://github.com/pytorch/pytorch/pull/109567#issuecomment-1738344950))', '[AOTInductor] ProxyExecutor supports custom op with tuple output (#110140)\n\nSummary:\nExtend ProxyExecutor to support custom ops with tuple outputs.\n\nGenerated wrapper code for `out3, out4 = torch.ops.fb.fn_with_tuple_output(out2, 1)`\n\n```\n    AtenTensorHandle buf5_handle;  // output buffer\n    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&buf5_handle));\n    RAIIAtenTensorHandle buf5(buf5_handle);\n    AtenTensorHandle buf6_handle;  // output buffer\n    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&buf6_handle));\n    RAIIAtenTensorHandle buf6(buf6_handle);\n    AtenTensorHandle tensor_args_var_3[] = {buf3.get(), buf5.get(), buf6.get()};\n    int64_t int_args_var_4[] = {1};\n    aoti_torch_proxy_executor_call_function(proxy_executor, 1, 1, int_args_var_4, 3, tensor_args_var_3);\n```\n\nTest Plan: Test\n\nDifferential Revision: D49673994\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110140\nApproved by: https://github.com/chenyang78', 'Enable typechecking for _inductor/fx_passes/group_batch_fusion.py (#110111)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110111\nApproved by: https://github.com/eellison, https://github.com/Skylion007\nghstack dependencies: #110109', 'Reorganize and rename COW files and APIs (#110191)\n\nThis PR does the following:\n* Combine `cow/context.<h/cpp>` and `cow/deleter.<h/cpp>` into `cow/COWDeleter.<h/cpp>`\n* Rename `Context` to `COWDeleterContext`\n* Rename `delete_context` to `cow_deleter`\n* Remove the separate `impl_cow_context` bazel library, combining it with the base c10 core library\n* Rename `context_test.cpp` to `cow_test.cpp`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110191\nApproved by: https://github.com/ezyang'], language:'Python' 

根据你给出的输入内容，我需要从提交记录中提取技术领域分类的特征范式。首先，我会分析提交记录中的内容，找出与技术领域相关的关键词和主题，然后将其归类到相应的技术领域中。

从给定的输入内容来看，主要涉及以下几个技术领域：

1. **错误修复（Bug Fixes）**：
   - 修复了多个运行时错误，如 `RuntimeError` 由 `torch.nn.modules.activation.MultiheadAttention` 引起。
   - 修复了安装问题，如在 Amazon Linux 2 上通过 pip 安装的回归问题。
   - 修复了与 ONNX 相关的问题，如 ATen Fallback 和 operator_export_type。

2. **性能优化（Performance Optimization）**：
   - 添加了对 fp16 的支持，提升了 gemm 操作的性能。
   - 优化了 ONNX 的性能测试，设置了正确的 CUDA 设备。

3. **代码改进（Code Improvements）**：
   - 添加了新的分解（decompositions）到核心 ATen 分解表中，如 `lift_fresh`, `split.Tensor`, 和 `unbind`。
   - 重构和重命名了 COW（Copy-On-Write）相关的文件和 API。

4. **测试和构建（Testing and Build）**：
   - 修复了构建问题，如在 Mac M1 上使用 poetry 安装的失败。
   - 添加了对 S3 凭据的传递，以解决上传问题。
   - 回滚了某些更改，以解决构建失败的问题。

5. **文档和注释（Documentation and Comments）**：
   - 补充了 masked tensor 的文档。

6. **安全性和注入（Security and Injection）**：
   - 修复了 `torch.jit.annotations.parse_type_line` 的命令注入问题。

7. **API 和接口（API and Interfaces）**：
   - 添加了新的 API，如 `Work::getSequenceNumber`。
   - 允许手动注册内核，以支持某些嵌入式系统。

8. **工具和基础设施（Tools and Infrastructure）**：
   - 使用了新的工具，如 SARIF 诊断信息的生成。
   - 改进了上传工作流，以支持 iOS 构建。

通过这些分析，可以提取出与技术领域分类相关的特征范式，包括错误修复、性能优化、代码改进、测试和构建、文档和注释、安全性和注入、API 和接口以及工具和基础设施等方面的内容。
好的，我将根据提供的提交记录内容，提取与技术领域分类相关的特征范式。以下是提取结果：

### 技术领域分类特征范式

1. **错误修复（Bug Fixes）**：
   - **运行时错误修复**：修复 `torch.nn.modules.activation.MultiheadAttention` 在 `bias=False` 和 `batch_first=True` 时的 `RuntimeError`。
   - **安装问题修复**：解决 Amazon Linux 2 上通过 pip 安装的回归问题，以及 Mac M1 上使用 poetry 安装的失败。
   - **ONNX 相关修复**：修复 ONNX 的 ATen Fallback 和 `operator_export_type` 问题。

2. **性能优化（Performance Optimization）**：
   - **FP16 支持**：为 gemm 操作添加 FP16 支持，提升在 Intel SPR 上的性能。
   - **ONNX 性能改进**：设置正确的 CUDA 设备以优化多设备 ONNX 性能测试。

3. **代码改进（Code Improvements）**：
   - **分解添加**：将 `lift_fresh`、`split.Tensor` 和 `unbind` 的分解添加到核心 ATen 分解表。
   - **COW 重构**：重构和重命名 COW（Copy-On-Write）相关的文件和 API，如 `COWDeleter` 和 `COWDeleterContext`。

4. **测试和构建（Testing and Build）**：
   - **构建修复**：解决 Mac M1 上 poetry 安装失败的问题。
   - **上传修复**：传递 S3 凭据以解决 iOS 上传问题。
   - **回滚更改**：回滚 Dynamo 跳过文件重构，以解决构建失败问题。

5. **文档和注释（Documentation and Comments）**：
   - **文档补充**：补充 masked tensor 的文档。

6. **安全性和注入（Security and Injection）**：
   - **命令注入修复**：修复 `torch.jit.annotations.parse_type_line` 的命令注入漏洞。

7. **API 和接口（API and Interfaces）**：
   - **新 API 添加**：引入 `Work::getSequenceNumber` 以支持集体操作的序列号。
   - **手动内核注册